{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection d'objets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préambule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dossier racine doit contenir:  \n",
    "-ce notebook  \n",
    "-le dossier ImageAI  \n",
    "-le dossier Cascade tel qu'il est donné dans le TP6 (donc avec ses propres réseaux)  \n",
    "-le réseau de neurone pré-entrainé pour ImageAI  \n",
    "-la vidéo road.mp4  \n",
    "-2 dossiers pré-créés (path, et detectedpath) qui conserveront les images durant l'execution.  \n",
    "  \n",
    "Afin de ne pas surcharger la taille du projet lors du rendu, seuls ce notebook et les dossiers d'images (pleins) seront rendus (afin de conserver une trace d'une execution fonctionnelle et de ne pas perdre du temps à executer ce notebook pendant une heure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cœur du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer ImageAI (assez lourde)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#Chemin de ce dossier (à changer)\n",
    "project_path= os.path.abspath(\"/Python/PI_TP7_Detection_objets/\")\n",
    "sys.path.append(project_path)\n",
    "\n",
    "# Mettre le chemin où vous avez placé la bibliothèque ImageAI\n",
    "imageai_path = os.path.join(project_path,\"ImageAI\") \n",
    "sys.path.append(imageai_path)\n",
    "from imageai.Detection import ObjectDetection\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on importe les dépendances de Cascade-LC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Pour charger les données et visualiser les résultats\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from matplotlib.pyplot import imshow, figure, subplots\n",
    "\n",
    "# Pour charger les réseaux neuronaux\n",
    "from Cascade.models.erfnet import Net as ERFNet\n",
    "from Cascade.models.lcnet import Net as LCNet\n",
    "\n",
    "\n",
    "from Cascade.functions import color_lanes\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit quelques variables globales afin de pouvoir executer les cellules séparément:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Les différents dossiers devront être créé avant l'execution.\n",
    "#Les images de la vidéo originales seront stockées dans le dossier:\n",
    "path = os.path.join(project_path,\"CapturedFrames\")\n",
    "\n",
    "#Les images finales (avec les objets détectés) sont envoyées dans le dossier :\n",
    "detectedpath = os.path.join(project_path,\"DetectedFrames\")\n",
    "\n",
    "# On ne traite pas toutes les images, donc on ne traitera que une image sur :\n",
    "frames_step = 20\n",
    "\n",
    "\n",
    "execution_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables propres à Cascade-LC\n",
    "DESCRIPTOR_SIZE = 64\n",
    "NUM_CLASSES_SEGMENTATION = 5\n",
    "NUM_CLASSES_CLASSIFICATION = 3\n",
    "\n",
    "#Chemin vers le réseau classifieur\n",
    "model_path = 'Cascade/pretrained/classification_{}_{}class.pth'.format(DESCRIPTOR_SIZE, NUM_CLASSES_CLASSIFICATION)\n",
    "#Chemin vers le réseau de segmentation\n",
    "seg_path = 'Cascade/pretrained/erfnet_tusimple.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ouvre la vidéo et on calcule le nombre d'images que l'on traitera (todo_frames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "# On ouvre la vidéo\n",
    "movie= cv2.VideoCapture(os.path.join(project_path, \"road.mp4\"))\n",
    "\n",
    "total_frames = movie.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "todo_frames = int(total_frames // frames_step)\n",
    "print(todo_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence en extrayant les frames de la vidéo que l'on souhaite traiter.  \n",
    "ATTENTION : Si le dossier path a déjà été rempli, il n'est pas obligatoire de le refaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(todo_frames):\n",
    "    movie.set(1, i * frames_step)\n",
    "    success, frame = movie.read()  \n",
    "\n",
    "    cv2.imwrite(os.path.join(path , 'img' + str(i) + '.jpg'), frame)\n",
    " \n",
    "movie.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère une des images capturée afin de récupérer les dimensions de la vidéo.\n",
    "imageTest=cv2.imread(os.path.join(path , 'img'+str(0)+'.jpg'))\n",
    "height,width,layers=imageTest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On défini un détecteur d'objets ImageAI et on lui donne le réseau de neurone pré-entrainé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "\n",
    "#On ajoute ici notre réseau pré-entrainé\n",
    "detector.setModelPath( os.path.join(execution_path , \"pretrained-yolov3.h5\"))\n",
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on charge le réseau de Cascade-LC avec pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des reseaux Cascade et chargement des poids\n",
    "segmentation_network = ERFNet(NUM_CLASSES_SEGMENTATION)\n",
    "classification_network = LCNet(NUM_CLASSES_CLASSIFICATION, DESCRIPTOR_SIZE, DESCRIPTOR_SIZE)\n",
    "\n",
    "segmentation_network.load_state_dict(torch.load(seg_path, map_location = map_location))\n",
    "classification_network.load_state_dict(torch.load(model_path, map_location = map_location))\n",
    "\n",
    "segmentation_network = segmentation_network.eval()\n",
    "classification_network = classification_network.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    segmentation_network = segmentation_network.cuda()\n",
    "    classification_network = classification_network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ajoute la fonction d'extraction des descripteurs (légèrement modifiée lors de l'initialisation de single_lane) qu'on appelera pour chaque image préalablement segmentées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptors(label, image):\n",
    "    # avoids problems in the sampling\n",
    "    eps = 0.01\n",
    "    \n",
    "    # The labels indices are not contiguous e.g. we can have index 1, 2, and 4 in an image\n",
    "    # For this reason, we should construct the descriptor array sequentially\n",
    "    inputs = torch.zeros(0, 3, DESCRIPTOR_SIZE, DESCRIPTOR_SIZE)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        \n",
    "    # This is needed to keep track of the lane we are classifying\n",
    "    mapper = {}\n",
    "    classifier_index = 0\n",
    "    \n",
    "    # Iterating over all the possible lanes ids\n",
    "    for i in range(1, NUM_CLASSES_SEGMENTATION):\n",
    "        # This extracts all the points belonging to a lane with id = i\n",
    "        #Cette ligne a été modifié car elle ne permettait pas à la fonction d'être appelée plus de 128 fois.\n",
    "        single_lane = label.eq(i).view(-1).nonzero(as_tuple=False).squeeze(1)\n",
    "        \n",
    "        # As they could be not continuous, skip the ones that have no points\n",
    "        if single_lane.numel() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Points to sample to fill a squared desciptor\n",
    "        sample = torch.zeros(DESCRIPTOR_SIZE * DESCRIPTOR_SIZE)\n",
    "        if torch.cuda.is_available():\n",
    "            sample = sample.cuda()\n",
    "            \n",
    "        sample = sample.uniform_(0, single_lane.size()[0] - eps).long()\n",
    "        sample, _ = sample.sort()\n",
    "        \n",
    "        # These are the points of the lane to select\n",
    "        points = torch.index_select(single_lane, 0, sample)\n",
    "        \n",
    "        # First, we view the image as a set of ordered points\n",
    "        descriptor = image.squeeze().view(3, -1)\n",
    "        \n",
    "        # Then we select only the previously extracted values\n",
    "        descriptor = torch.index_select(descriptor, 1, points)\n",
    "        \n",
    "        # Reshape to get the actual image\n",
    "        descriptor = descriptor.view(3, DESCRIPTOR_SIZE, DESCRIPTOR_SIZE)\n",
    "        descriptor = descriptor.unsqueeze(0)\n",
    "        \n",
    "        # Concatenate to get a batch that can be processed from the other network\n",
    "        inputs = torch.cat((inputs, descriptor), 0)\n",
    "        \n",
    "        # Track the indices\n",
    "        mapper[classifier_index] = i\n",
    "        classifier_index += 1\n",
    "        \n",
    "    return inputs, mapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction Cascade sera appelée dès qu'on souhaitera trouver des lignes dans une image, et coloriera cette dernière en conséquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La fonction blend() originale n'était pas compatible avec des images ouvertes par OpenCV\n",
    "def myOwnBlend(img,classificator):\n",
    "    height, width, colors = img.shape\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            for c in range(colors):\n",
    "                if classificator[i][j][c]:\n",
    "                    img[i][j][c] = classificator[i][j][c]\n",
    "    return img\n",
    "\n",
    "#Classification des lignes pour une image ouverte avec openCV\n",
    "def Cascade(im):\n",
    "    im_tensor = ToTensor()(im)\n",
    "    im_tensor = im_tensor.unsqueeze(0)\n",
    "    \n",
    "    out_segmentation = segmentation_network(im_tensor)\n",
    "    out_segmentation = out_segmentation.max(dim=1)[1]\n",
    "\n",
    "    descriptors, index_map = extract_descriptors(out_segmentation, im_tensor)\n",
    "    #On récupère les différentes classes(lignes différentes) grâce aux descripteurs.\n",
    "    classes = classification_network(descriptors).max(1)[1]\n",
    "    \n",
    "    #Puis on colorie l'image suivant les lignes que l'on trouve\n",
    "    out_segmentation_np = out_segmentation.cpu().numpy()[0]\n",
    "    \n",
    "    classification_picture = np.zeros((height,width, 3))\n",
    "    for i, lane_index in index_map.items():\n",
    "        if classes[i] == 0: # Continuous\n",
    "            color = (255, 0, 0)\n",
    "        elif classes[i] == 1: # Dashed\n",
    "            color = (0, 255, 0)\n",
    "        elif classes[i] == 2: # Double-dashed\n",
    "            color = (0, 0, 255)\n",
    "        else:\n",
    "            raise\n",
    "        classification_picture[out_segmentation_np == lane_index] = color\n",
    "    #image finale\n",
    "    finale = myOwnBlend(im, classification_picture)\n",
    "    return finale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique le détecteur d'objets et le réseau en cascade sur chaque image afin de reconnaitre des objets connus de notre réseau.  \n",
    "ATTENTION : Ce processus risque de prendre énormément de temps (17 secondes par image * todoframes => une heure), à ne faire que si le dossier detectedpath est vide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(todo_frames):\n",
    "    #Detection d'objets (une ligne).\n",
    "    tmp_image,ret = detector.detectObjectsFromImage(input_image = os.path.join(path , 'img' + str(j) + '.jpg'), \n",
    "                                    minimum_percentage_probability = 40,\n",
    "                                    output_type = 'array')\n",
    "    #Detection des lignes dans un second temps\n",
    "    finale_image = Cascade(tmp_image)\n",
    "    cv2.imwrite(os.path.join(detectedpath,'img' + str(j) + '.jpg'), finale_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On compile maintenant les nouvelles images avec les objets détectés sous la forme d'une vidéo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le 3eme paramètre est le framerate de la vidéo,vous pouvez l'augmenter si vous voulez que la vidéo soit plus rapide.\n",
    "#J'ai executé ce code sur MacOS, il est donc probable qu'il faille changer le codec fourcc par (*'XVID')\n",
    "video=cv2.VideoWriter('vehicles_detection.avi',cv2.VideoWriter_fourcc(*'DIVX'), 5,(width,height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On écrit les images une par une dans la vidéo\n",
    "for k in range(todo_frames):\n",
    "    image=cv2.imread(os.path.join(detectedpath , 'img'+str(k)+'.jpg'))\n",
    "    video.write(image)\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes :  \n",
    "On remarque vers les 3/5ièmes de la vidéo finale que les piétons sont correctement identifiés comme des personnes (on ne peut les voir que quelques images). La liste des objets reconnaissables est donnée dans le fichier _init_.py du dossier Detection de ImageAI.  \n",
    "Il arrive sur certaines images que l'avant de la voiture qui filme soit détecté. Il arrive aussi que certaines voitures lointaines soient reconnues en tant que camions (\"trucks\") ou en vaches, la faible résolution diminue évidemment la précision du réseau dans ce genre de cas.  \n",
    "Des maisons sont aussi reconnues comme étant des camions, ou des panneaux de directions comme des feux tricolores. Ce genre de mauvaise classifications est souvent considérée comme imprécise (moins de 75% de certitude).   \n",
    "La détection de lignes est assez imprécise par rapport à l'autre réseau, surtout quand il ne s'agit pas de lignes continues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
